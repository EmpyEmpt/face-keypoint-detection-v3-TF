{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import dlib\n",
    "import albumentations as A\n",
    "import cv2\n",
    "from imutils import face_utils\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.python.data import AUTOTUNE\n",
    "import yaml\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key=open('../secrets/wandb_key.txt', 'r').read(), relogin=True)\n",
    "config = yaml.safe_load(open('../config.yaml', 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset_path: str, image_size: int, maxi=np.Infinity):\n",
    "    images = np.empty([0, image_size, image_size, 3], dtype=np.uint8)\n",
    "    keypoints = np.empty([0, 68, 2], dtype=np.int16)\n",
    "    p = \"../shape_predictor_68_face_landmarks.dat\"\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(p)\n",
    "    directory = os.fsencode(dataset_path)\n",
    "\n",
    "    co = 0\n",
    "\n",
    "    transform = A.Compose(\n",
    "        [A.Rotate(p=0.6, limit=15),\n",
    "         #  A.RandomCrop(height=750, width=750, p=0.2),\n",
    "         A.HorizontalFlip(p=0.5),\n",
    "         A.ImageCompression(quality_lower=20, quality_upper=70, p=1),\n",
    "         A.GaussianBlur(blur_limit=(3, 13), sigma_limit=0, p=0.8),\n",
    "         A.RandomBrightnessContrast(p=0.4)\n",
    "         ],\n",
    "        keypoint_params=A.KeypointParams(\n",
    "            format='xy', remove_invisible=False)\n",
    "    )\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.fsdecode(file)\n",
    "        image = cv2.imread(dataset_path + filename)\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        rects = detector(gray, 0)\n",
    "        for (_, rect) in enumerate(rects):\n",
    "            shape = predictor(gray, rect)\n",
    "            shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "            transformed = transform(image=image, keypoints=shape)\n",
    "            image = transformed['image']\n",
    "            image = np.array(image, dtype=np.uint8)\n",
    "            shape = transformed['keypoints']\n",
    "            shape = np.array(shape, dtype=np.int16) / 1024 * image_size\n",
    "            shape = shape.astype(dtype=np.uint8)\n",
    "\n",
    "            image = cv2.resize(image, (image_size, image_size),\n",
    "                               interpolation=cv2.INTER_AREA)\n",
    "            # image = image / 255\n",
    "\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "            images = np.append(images, image, axis=0)\n",
    "\n",
    "            shape = np.expand_dims(shape, axis=0)\n",
    "            keypoints = np.append(keypoints, shape, axis=0)\n",
    "\n",
    "            break\n",
    "        co += 1\n",
    "        if co > maxi:\n",
    "            break\n",
    "    return images, keypoints\n",
    "\n",
    "\n",
    "def compress_splits(X, Y, dir):\n",
    "    np.savez_compressed(dir + 'Xvalues.npz', X)\n",
    "    np.savez_compressed(dir + 'Yvalues.npz', Y)\n",
    "\n",
    "\n",
    "def uncompress_splits(dir: str):\n",
    "    X = np.load(dir + 'Xvalues.npz')['arr_0']\n",
    "    Y = np.load(dir + 'Yvalues.npz')['arr_0']\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def split_dataset(X, Y, test_ratio: float = 0.20):\n",
    "    size = int(len(X) * test_ratio)\n",
    "    return X[size:], X[:size], Y[size:], Y[:size]\n",
    "\n",
    "\n",
    "def normalize_images(images):\n",
    "    images /= 255\n",
    "    return images\n",
    "\n",
    "\n",
    "def normalize_keypoints(keypoints, image_size):\n",
    "    keypoints /= image_size\n",
    "    return keypoints\n",
    "\n",
    "\n",
    "def preprocess(images, keypoints, image_size):\n",
    "    images = normalize_images(images)\n",
    "    keypoints = normalize_keypoints(keypoints, image_size)\n",
    "    return images, keypoints\n",
    "\n",
    "\n",
    "def fetch_ds(config, op_type='train'):\n",
    "    # load dataset\n",
    "    images, keypoints = uncompress_splits(config['dataset']['compressed_dir'])\n",
    "\n",
    "    # preprocess ds\n",
    "    images, keypoints = preprocess(images, keypoints, config['img_shape'])\n",
    "\n",
    "    # split ds\n",
    "    images, keypoints = shuffle(images, keypoints, random_state=0)\n",
    "    train_x, test_x, train_y, test_y = split_dataset(\n",
    "        images, keypoints, config['dataset']['split_ratio'])\n",
    "\n",
    "    # put into tf.ds\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (train_x, train_y))\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y))\n",
    "\n",
    "    # visualization\n",
    "    # log_image_artifacts_to_wandb(data=train_ds, metadata=metadata)\n",
    "\n",
    "    train_dataset = train_dataset.batch(config[op_type]['batch_size'])\n",
    "    train_dataset = train_dataset.cache()\n",
    "    train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    test_dataset = test_dataset.batch(config[op_type]['batch_size'])\n",
    "    test_dataset = test_dataset.cache()\n",
    "    test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return train_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 192\n",
    "data_dir = '../data/'\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER_SIZE = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(project=config['wandb']['project'],\n",
    "           name='Dataset',\n",
    "           config=config):\n",
    "    i, k = create_dataset(dataset_path=data_dir, image_size=image_size)\n",
    "    compress_splits(i,k, '../data/')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = fetch_ds(config, 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "from src.data_tests import pass_tests_before_fitting\n",
    "from src.model import compile_model\n",
    "from src.dataset import fetch_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(project=config['wandb']['project'],\n",
    "           name=config['wandb']['name'],\n",
    "           config=config):\n",
    "    model = compile_model(\n",
    "        input_shape=config['img_shape'], output_shape=config['kp_shape'])\n",
    "\n",
    "    callbacks = [EarlyStopping(**config['callbacks']['EarlyStopping']),\n",
    "                ReduceLROnPlateau(**config['callbacks']['ReduceLROnPlateau']),\n",
    "                WandbCallback(**config['callbacks']['WandbCallback'])]\n",
    "\n",
    "    # data tests (pre-fitting)\n",
    "    pass_tests_before_fitting(\n",
    "        data=train_dataset, img_shape=config['img_shape'], keypoint_shape=config['kp_shape'])\n",
    "    pass_tests_before_fitting(\n",
    "        data=test_dataset, img_shape=config['img_shape'],  keypoint_shape=config['kp_shape'])\n",
    "\n",
    "    # training\n",
    "    history = model.fit(\n",
    "        train_dataset, epochs=config['train']['epochs'], validation_data=test_dataset, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "\n",
    "from src.data_tests import pass_tests_before_fitting\n",
    "from src.model import compile_model\n",
    "from src.dataset import fetch_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(project=config['wandb']['project'],\n",
    "                name='Optimization',\n",
    "                config=config):\n",
    "\n",
    "    prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "    # Compute end step to finish pruning after 2 epochs.\n",
    "    tf.random.set_seed(config['random_seed'])\n",
    "\n",
    "    train_dataset, test_dataset = fetch_ds(config, 'optimize')\n",
    "\n",
    "    # Define model for pruning.\n",
    "    # TODO: WHAT ARE THOOOOOSE\n",
    "    end_step = np.ceil(\n",
    "        config['amount'] / config['optimize']['batch_size']).astype(np.int32) * config['optimize']['epochs']\n",
    "    pruning_params = {\n",
    "        'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                                 final_sparsity=0.80,\n",
    "                                                                 begin_step=0,\n",
    "                                                                 end_step=end_step)\n",
    "    }\n",
    "\n",
    "    model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "    # `prune_low_magnitude` requires a recompile.\n",
    "    model_for_pruning = compile_model(\n",
    "        input_shape=config['img_shape'], output_shape=config['kp_shape'], model=model)\n",
    "\n",
    "    # model_for_pruning.summary()\n",
    "    logdir = tempfile.mkdtemp()\n",
    "    callbacks = [\n",
    "        tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "        tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "    ]\n",
    "\n",
    "    #  TODO: wandbcallback with tfds???\n",
    "    callbacks = [tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "                 tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "                 EarlyStopping(**config['callbacks']['EarlyStopping']),\n",
    "                 ReduceLROnPlateau(**config['callbacks']['ReduceLROnPlateau']),\n",
    "                 WandbCallback(**config['callbacks']['WandbCallback'])]\n",
    "\n",
    "    # data tests (pre-fitting)\n",
    "    pass_tests_before_fitting(\n",
    "        data=train_dataset, img_shape=config['img_shape'], keypoint_shape=config['kp_shape'])\n",
    "    pass_tests_before_fitting(\n",
    "        data=test_dataset, img_shape=config['img_shape'],  keypoint_shape=config['kp_shape'])\n",
    "\n",
    "    # training\n",
    "    history = model_for_pruning.fit(\n",
    "        train_dataset, epochs=config['optimize']['epochs'], validation_data=test_dataset, callbacks=callbacks)\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
